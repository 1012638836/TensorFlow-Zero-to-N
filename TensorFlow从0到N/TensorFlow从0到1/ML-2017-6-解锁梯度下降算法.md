# 6 解锁梯度下降算法

上一篇[5 TF轻松搞定线性回归](./ML-2017-5-TensorFlow轻松搞定线性回归.md)，我们知道了模型参数训练的方向是由梯度下降算法指导的，并使用TF的封装`tf.train.GradientDescentOptimizer(0.01)`（学习率为0.01）完成了机器自学习的过程。本篇开启梯度下降算法的黑盒一探究竟，并解锁几个TF API常用参数的真正含义：

- learning rate；
- steps；
- epoch；
- batch。

![雪山速降](img/2017-6-skiing.jpg)

## 一般函数的最小值问题

[4 第一个机器学习问题](./ML-2017-4-第一个机器学习问题.md)引入了损失函数的定义，即待训模型参数为自变量，计算模型输出的均方差。函数C(a，b)的最小值处的(a, b)值即我们要找的模型参数的最优解。

![B-O-F-1 损失函数](img/2017-B-O-F-1.jpg)

本节将之前损失函数自变量a和b一般化表示为v<sub>1</sub>，v<sub>2</sub>，把求解损失函数的最小化问题，转换为更一般的函数**C(v<sub>1</sub>,v<sub>2</sub>)**最小化问题，C(v<sub>1</sub>,v<sub>2</sub>)具有任意的函数形式。如果找到一般的函数最小值求解方法，那么具有特殊形式的损失函数最小值求解自不在话下。

对于C是一个或者少数几个变量的函数，可以通过函数极值点处的导数特性来获得多元方程组，直接求解极值点。但是我们准备放弃这种尝试，因为对于一个真实世界的机器学习问题，其模型的复杂程度通常会远远的高于线性模型，参数的个数远不止两个，损失函数的形式会变成：**C(v<sub>1</sub>, v<sub>2</sub> ... v<sub>n</sub>)**，如果n数以亿计，用微积分的方法简直就是噩梦。

## 雪山速降的启发

把损失函数想象成前面图中的雪山，直觉上速降的最佳路径就是沿着雪山**最陡峭**的方向下山。

如果我们不能直接看出函数的最小值，或者通过直接求解的方式得到函数最小值，那么利用雪山速降的启发，总是沿着**最陡峭**的下降方向移动，就会最快到达最小值点。

回到数学的角度，考虑有两个自变量的二次函数C(v<sub>1</sub>, v<sub>2</sub>)，它是一个曲面。假设有个小球靠自身重力滚落到曲面的底部，可以想象其路径也是沿着“最陡峭”的方向的。

那么“最陡峭”在数学上的表达是什么呢？

![梯度下降](img/2017-6-GD.jpg)

## 梯度的定义

微积分告诉我们，当把v<sub>1</sub>，v<sub>2</sub>，...， v<sub>n</sub>各个自变量移动一个很小的值，C将有如下变化：

![B-C-F-1 微积分](img/2017-B-C-F-1.jpg)

**梯度**定义有：

![B-C-F-2 梯度](img/2017-B-C-F-2.jpg)

v的变化量为∆v ≡ (∆v<sub>1</sub>, ∆v<sub>2</sub>, ..., ∆v<sub>n</sub>)<sup>T</sup>，则C的变化量可重写为梯度向量▽C与v的变化向量∆v的点乘：

![B-C-F-3 C的增量](img/2017-B-C-F-3.jpg)

## 梯度下降算法

直觉上，如果v朝某个方向上移动，导致C的增量是个负数，那么就说明C在“下降”。

开下脑洞，直接令**∆v = -η▽C**，其中η是一个正数，代入公式B-C-F-3有：

**∆C ≈ -η▽C·▽C = -η‖▽C‖<sup>2</sup> ≤ 0**，此时∆C一定小于等于0，C在下降。

幸运的是，数学上可以证明对于一个非常小的固定步长，∆v = -η▽C可以使C的减小最大化。这就是说，**-η▽C**是我们期望v移动的正确方向！其中**η是学习率learning rate**。

“最陡峭的一小步”的数学解释就是**沿着梯度的负方向上走一小步**。“梯度下降”，名副其实。

只要一小步一小步朝着正确的方向移动，迟早可以走到C(v<sub>1</sub>, v<sub>2</sub>, ..., v<sub>n</sub>)的最小值处。

梯度下降的具体操作方法如下：

1.	随机选取自变量的初始位置v（以后会专门讨论初始化的技巧）；
2.	**v → v' = v - η▽C<sub>v</sub>**（v移动到v'，▽C<sub>v</sub>是v处的梯度值，η保持不变）；
2.  **v' → v'' = v' - η▽C<sub>v'</sub>**（v'移动到v''，▽C<sub>v'</sub>是v'处的梯度值，η保持不变）；
3. ...

**v移动的次数，即训练的步数steps**。

v是各个自变量(v<sub>1</sub>, v<sub>2</sub>, ..., v<sub>n</sub>)的向量表示，那具体到每个自变量该如何移动呢？以v<sub>1</sub>，v<sub>2</sub>为例：

![B-O-F-3 分量的增量](img/2017-B-O-F-3.jpg)

## 随机梯度下降算法

到此，梯度下降算法解决了如何寻求一般函数C(v<sub>1</sub>, v<sub>2</sub>, ..., v<sub>n</sub>)的最小值问题（这个算法在有些情况下会失效，会在后面讨论），那么马上应用到机器学习吧。可是别急，还差一小步。

![B-O-F-2 损失函数](img/2017-B-O-F-2.jpg)

回到损失函数，再仔细看看其形式，发现它有个特别之处，即函数表达式与训练样本集密切相关。原因是它是每个样本方差的累加，最后再求均值。训练样本集通常成千上万，为了求取▽C难道真的需要先代入所有训练样本吗？

实践中，其实不是这样的，而是有更加巧妙的方法：

![B-O-F-4 样本梯度均值](img/2017-B-O-F-4.jpg)

损失函数的梯度▽C，可以通过单个样本梯度值▽C<sub>x</sub>的均值得到。计算单个样本的梯度值▽C<sub>x</sub>是相对容易的。如果你对这个公式持怀疑态度，这不奇怪，一个简单的消除疑虑的做法就是用之前的线性模型和损失函数，用两个样本值分别计算一下等式两边，看是否相等即可。

可即便如此，对于样本集成千上万个样本，对每个样本x都求其▽C<sub>x</sub>，计算量还是太大了。假如故意减少样本数量会怎么样呢？也就是说，用一个小批量样本，通过其中每个样本▽C<sub>x</sub>的均值，来近似计算▽C：

![B-O-F-5 样本梯度均值的近似](img/2017-B-O-F-5.jpg)

这就是实践中采用的方法，被称为**随机梯度下降法**。那个**小批量样本就是一个batch**。

**把全部样本集分成一批批的小样本集，每全部遍历使用过1次，就称为1次迭代，即epoch**。

据此，每个自变量更新的公式如下：

![B-O-F-6 分量的增量](img/2017-B-O-F-6.jpg)

